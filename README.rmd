---
title: "Data Analysis Supplement"
output:
  html_document:
    highlight: null
    theme: lumen
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
date: "April 11, 2019"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval=FALSE)
```

## Introduction

This is a beginner's guide for analysis of spatial transcriptomic data. It 
fills in a few of the blanks in the manufacturer's documentation on github, 
since it is usually impossible to fully anticipate and simulate what a true 
novice will encounter on their first attempt at using an informatics 
pipeline.

This tutorial assumes you have at least 1 pair of fastq files generated from 
spatial transcriptomics libraries. The following steps will take you through 
setting up an analysis environment (such as within a docker image, an AWS 
instance, or other unix-flavored server to which you have access.)

## Pre-requisites

We start with a bare Ubuntu Ubuntu Server 18.04 LTS (HVM) instance, 
ami-01b60a3259250381b. Our first step is just to create the conda 
environment, but also to index the genome, so we will begin with a 
m5.4xlarge instance type with 500GB of storage.

We will create our usual build environment, with everything going into 
`~/local` and building occuring in `~/build` such that root access would 
not be necessary (assuming someone has created a sane build environment). 
After setting up the directory structure, we install 
miniconda to encapsulate the rest of our installation and maintain sanity

```{bash} 
# if not already done
sudo apt-get update
sudo apt-get install -y build-essential zlib1g-dev libbz2-dev liblzma-dev \
      libssl-dev libcurl4-openssl-dev libgfortran-8-dev


# if you don't have these already
mkdir ~/local
mkdir ~/local/share
mkdir ~/local/share/genomes

mkdir ~/build

# download a genome
cd ~/local/share/genomes
wget ftp://ftp.ensembl.org/pub/release-96/fasta/mus_musculus/dna/Mus_musculus.GRCm38.dna_sm.primary_assembly.fa.gz
wget ftp://ftp.ensembl.org/pub/release-96/gtf/mus_musculus/Mus_musculus.GRCm38.96.gtf.gz
gunzip *.gz

# Install miniconda (NOT miniconda2) into ~/local/share/miniconda
cd ~/build
wget https://repo.continuum.io/miniconda/Miniconda-latest-Linux-x86_64.sh
chmod 755 Miniconda-latest-Linux-x86_64.sh
# run the installer. When prompted, tell it to install into 
~/local/share/miniconda, and agree to add it to your PATH
./Miniconda-latest-Linux-x86_64.sh

# start a new bash to pickup the updated PATH from the Miniconda install
bash
conda config --add envs_dirs '~/local/share/miniconda/envs'

```

## Creating conda environment

Now we can create our environment with the pipeline and associated tools.

```{bash} 
# TO DO: Some of these dependencies may be unnecessary
conda create -p ~/local/share/miniconda/envs/spaceshell python  \
    scipy pandas pyyaml matplotlib pip 

source activate spaceshell
pip install --upgrade pip
conda install -c anaconda numpy 

# this may not be necessary with anaconda numpy installed...
conda install libgfortran==1

conda install -c bioconda star

```

And install the pipeline

```{bash} 
pip install stpipeline

```

Next we need to generate a STAR index for the genome

```{bash}
STAR --runThreadN 15 \
    --runMode genomeGenerate \
    --genomeDir ~/local/share/genomes \
    --genomeFastaFiles ~/local/share/genomes/Mus_musculus.GRCm38.dna_sm.primary_assembly.fa \
    --sjdbGTFfile ~/local/share/genomes/Mus_musculus.GRCm38.96.gtf 

```

## Staging data files

If your data files are not already on the server where you are running the 
pipeline, you will need to move them to that server.  The simplest way to do 
this is with `scp`.

Here we assume that the 
image files are in a directory called "ST1", and that the fastqs are in 
subdirectory or subdirectories of ST1, one folder per flowcell. The 
destination folder in this example is /home/ubuntu/projects/ST1/. The 
following would be run from a terminal on whatever server the data files 
are currently stored on. Replace `99.99.99.999` with the public IP address 
of the analysis server (e.g. the AWS instance where you are doing the 
analysis), and "your_key.pem" with the name of the SSH key file for the 
destination server. If you do not require a key file to access the server 
(i.e. if it is on your local network), you can omit that option.

```{bash}
scp -r -i .ssh/your_key.pem ST1/* \
    ubuntu@99.99.99.999:/home/ubuntu/projects/ST1/
```

## Merging flow cells

If each sample is spread accross multiple libraries, you can merge them with 
merge_fastq.py. However, in our case, we had one library per sample but 
it was sequenced on multiple flowcells. So we need to merge them by hand. Here 
the subdirectories containing the fastqs were `FlowCell1` and `FlowCell2`.

```{bash}
cd FlowCell1
ls *.gz | xargs -n1 -P15 gunzip
cd ../FlowCell2
ls *.gz | xargs -n1 -P15 gunzip
cd ..
mkdir merged
ls FlowCell1 | xargs -n1 -P12 -I file sh -c \
    'cat FlowCell1/"file" FlowCell2/"file" > merged/"file"'

```

## Mapping counts to genome and spot coordinates

Now we are ready to run the pipeline. Note that we first fetch the index file that 
corresponds to the Lot # for our slide.  This is printed on the slide and 
is of the format "Lot # 1000N", where N is a number 2-9 (as of this writing).
In the example below we fetch the id file for Lot #10003. 

We have adjusted the clipping and trimming arguments to account for the fact 
that our dataset was generated with the 75BP sequencing kit. If the 150bp kit 
is used, you can probably remove the `--disable-clipping` and 
`--min-length-qual-trimming` lines below.


```{bash}
wget https://raw.githubusercontent.com/SpatialTranscriptomicsResearch/st_pipeline/master/ids/1000L3_barcodes.txt

mkdir out
st_pipeline_run.py \
    --expName 3MI \
    --ids 1000L3_barcodes.txt \
    --disable-clipping \
    --min-length-qual-trimming 20 \
    --ref-map ~/local/share/genomes/ \
    --log-file log_file.txt \
    --output-folder out \
    --ref-annotation  ~/local/share/genomes/Mus_musculus.GRCm38.96.gtf \
    --verbose \
    merged/3MI_L000_R1_001.fastq merged/3MI_L000_R2_001.fastq
    
```

